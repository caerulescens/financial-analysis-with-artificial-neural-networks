%! Author = caerulescens
%! Date = 4/7/2017

% Preamble
\documentclass[../main.tex]{subfiles}

% Document
\begin{document}

    The results and ideas presented within this paper would not be possible without the contributions from researchers of the past, so this chapter presents previous research on market analysis.
    The concepts of time series analysis and artificial neural networks have been detailed, but the advantages and disadvantages of either one has not been discussed.
    Many questions arise when forecasting short-term stock returns like is the stock market random or predictable?
    Do time series analysis methods outperform machine learning methods?
    Which neural network weight optimization method creates better forecasts?
    These questions are answered through the work of other researchers, and all decisions made in chapter seven have been based upon the successes and failure of others.
    When discussing financial forecasting, there are three beliefs which researchers commonly hold towards the methods and results of for profit market analysis.

    One school of thought asserts that no one can achieve better than average accuracy in predicting the direction or level of change in the stock market.
    Fama created the Efficient Market Hypothesis to reflect this idea, and Jensen believes that there is concrete proof that the Efficient Market Hypothesis holds. 15,16
    The Efficient Market Hypothesis states that all markets are extremely efficient, meaning all the information of a stock is already reflected in its price; it’s impossible to profit from signal analysis and model building similar to the methods presented within chapter two and three.
    This belief was widely debated within the financial community in the 1970s, but many academic articles have found results contradicting this theory.
    The Efficient Market Hypothesis is commonly used to argue that financial markets follow a Random Walk Model; this is called the Random Walk Hypothesis.

    Lo and MacKinlay tested the random walk hypothesis for weekly stock market returns by comparing variance estimators derived from data sampled at different frequencies.
    They concluded that the random walk model was strongly rejected for the entire sample period and for all sub-periods for a variety of aggregate return indexes and size-sorted portfolios. 17
    Similarly, Lendasse, et. al., had the goal of breaking the Random Walk Hypothesis with non- linear statistical methods, and they were able to create an artificial neural network model which found non-linear relationships within the stock data. 18
    Hassan, Nath, and Kirley used Hybrid Hidden Markov, artificial neural network, and genetic algorithm models with the goal of forecasting three major stocks; their results indicated that the next day’s closing price could be predicted within 2\% of the actual value of the stock. 19
    Post Fama and Jensen literature indicates that the Efficient Market Hypothesis fails to hold when machine learning methods are used and stock markets do not follow random walks.

    The second type of financial analyst encourages fundamental macroeconomic analysis of financial environments and has the goal of finding correlations between exogenous variables.
    This thesis does not address the macroeconomic methods that are used to forecast financial markets.
    This type of analysis is synonymous with forecasting by humans, but not surprisingly, humans are bias in their predictions.
    O’Connor, Remus, and Griggs had humans forecast artificial time series to discover if the trend’s direction effected forecasting accuracy.
    They determined that “People were found to be surprisingly inaccurate,” and that they had significant difficulties in dealing with downward-sloping series; the study showed that the direction of a time series’ trend makes a significant difference in the accuracy of a forecast. 20
    Mosteller et al. and Edmendson found that people are able to draw best fit lines which are significantly close to a least-squares regression method, which says that people can assess trends very well. 21
    In contrast, Collyer, Standley, and Bowater found that people tended to merely bisect the scatter plots. 22
    People are less accurate and efficient when compared to computer centered approaches.

    The tertiary view focused on applying time series analysis and machine learning methods based upon rigorously proven mathematics with the intention of forecasting financial returns.
    Time series analysis methods like ARIMA and GARCH along with machine learning concepts like support vector machines, artificial neural networks, genetic algorithms, fuzzy logic, and more all fall into this category.
    The results from numerous studies stemming from this school of thought have evidence against the Efficient Market Hypothesis.

    Concerning which method of analysis results in higher success, articles published across many journals assert that machine learning models consistently outperform time series analysis models.
    Hamzaçebi, Akay, and Kutav compared direct and iterative methods classifying with neural networks against multiple methods.
    They asserted that ARIMA was the worst performing of the seven methods they compared while the artificial neural network performed the best. 23
    Yümlü, Gürgen, and Okay used four different models including EGARCH and multi-layered neural network models to predict the ISE-XU-100 daily values, and over the four-years of testing data, EGARCH performed the worst. 24
    Hassan, Nath, and Kirley compared a hybrid neural network model against ARIMA when forecasting Apple, IBM, and
    Dell’s stocks; they only tested over five weeks of data, but the neural network model beat ARIMA. 25
    Hamzaçebi and Bayramoǧlu used ARIMA and artificial neural networks to forecast daily closing prices of the ISE-XU-100, and the neural network produced significantly better results.
    Even when neural network and time series analysis methods were combined into hybrid models like in Roh’s paper, the pure neural network model outperformed the hybrid models.
    The list of articles which detail neural networks forecasting more accurately than ARIMA, GARCH, or similar methods are too numerous to list in this thesis.
    Since academic literature has empirically determined the success of utilizing artificial neural networks, the focus changes to determining the optimal neural network model for forecasting daily closing prices.

    Most of the benefits of neural networks come from rigorously proven theorems about the abilities and limitations of neural networks of different sizes.
    The most important theorem is the Universal Approximation Theorem which says that any continuous function can be uniformly approximated by a continuous neural network having only one internal hidden layer and with an arbitrary continuous sigmoidal nonlinearity. 27
    Because of this theorem and from the successes of other researchers, a single hidden layer neural network is used for financial forecasting of short term stock returns.

    There are two different approaches one may take when training and testing an artificial neural network.
    The direct method trains and validates on historical data and does not update the weights of the network during testing, while the iterative method differs by updating the network to reflect new information after the day is forecasted.
    A Bayesian would say that any new information would only benefit the accuracy of the network and should be utilized, but what other researchers found contradicts this belief.
    Hamzacebi, Akay, and Kutay compared direct and iterative methods by creating neural networks for both approaches and forecasting stock returns; they determined that “the direct method is superior.”
    Zhang supports these results with his conclusion that the direct method is better for forecasting. 29
    In contrast, Weigend, Huberman, and Rumelhart showed that iterative forecasting results in better accuracy when analyzing the sunspot data set. 30
    Intuitively, an iterative method would be necessary for real time forecasting, but it may not matter for daily forecasting.
    For simplicity, the direct method is used for forecasting daily stock returns and is explained thoroughly in chapter seven.

    Artificial neural networks are trained with the specific goal of level estimation or classification, where a estimation network forecasts the magnitude of a stock, and a classifier network predicts the sign of change.
    Some investors only care about the direction of change, others will want to know the magnitude of change, and the rest will want to know both.
    The question naturally arises, which method is more accurate and does any success translate to profit?
    Leung, Daouk, and Chen compared multiple models and developed threshold trading rules with the goal of profiting.
    Their experiment suggested that the classification models outperform the level estimation models in terms of predicting the direction of the stock market movement and maximizing returns from investment trading. 31
    Kara, Boyacioglu, and Baykan also developed two efficient models for classification and level estimation, and they concluded that the classification model outperformed the level estimation model in terms of predicting the direction of the stock market movement and maximizing returns from investment trading. 32

    The classification neural network method can be thought of as a specialized level estimation neural network because the level estimation network will forecast the magnitude and indirectly the direction of change, but the classification network cannot do both.
    Since artificial neural networks are especially good at forecasting nonlinear spikes and crashes, level estimation is used for the models presented in chapter seven.
    The classification and level estimation proficiency of these models are quantified in chapter eight and discussed in chapter nine.

    If profitability is the main goal, many researchers are able to forecast the sign of change for daily closing prices significantly higher than the Efficient Market Hypothesis’ implied 50%accuracy.
    Kara, Boyacioglu, and Baykan created support vector machine and artificial neural network models for forecasting the ISE National 100 Index and achieved classification accuracies of 71.52\% and 75.74\% on average, respectively.
    Diler used technical indicators to train his back-propagation artificial neural network, and his results showed that the direction of the ISE National 100 Index could be forecasted with a rate of 60.81\%.
    Rodriguez,Gonzalez-Martel, and Sosvilla-Rivero used neural networks to determine the profitability of trading in security markets.
    Their results indicated that the neural network based trading strategies they developed, when applied to the General Index of the Madrid stock exchange, are always superior to a buy-and-hold strategy for bear and stable markets; this was in the absence of trading costs. 34

    If used correctly, artificial neural networks are proficient at classification and level estimation for many different domains, but they have disadvantages as well.
    Guresen and Kayakutlu said that artificial neural networks are popular for complex financial markets, but they claim that noise caused by changes in market conditions makes it hard to reflect the market variables directly into the models without any assumptions.
    Similarly, Kim asserted that “ANN often exhibits inconsistent and unpredictable performance on noisy data,” and stock markets are noisy systems. 36
    Other issues with neural networks come from their ability to learn trained data extremely well but this does not imply good results from testing with unseen data.
    Ticknor says that one drawback of using standard back-propagation networks is the potential for overfitting the training data set, which results in reduced accuracy on unknown test sets. 37

    Another negative aspect of back-propagation trained neural networks is the amount of parameters which must be optimized for gradient descent to converge to a local or global minimum solution.
    For back-propagation, this includes the learning rate, hidden-layer nodes,momentum, activation function, weight optimization method, and any additional parameters that optimization method uses.
    Li, et. al compared extreme learning machines to back- propagation trained neural networks.
    Back-propagation theory stresses that the parameters of hidden-layer nodes needs to be greater than or equal to the input nodes which is 1,011 features in their problem.
    They found that after several trials of running, it was impossible for their servers to support that many nodes since back-propagation training required too much memory.
    The authors proposed an extreme learning machine model for forecasting stock returns instead of the traditional back-propagation neural networks.

    The subject of debate within this paper is whether extreme learning machines offer a significant advantage over the traditional method of forecasting with back-propagation trained neural networks.
    The extreme learning machine model was originally proposed by Huang et al. from Nanyang University in Singapore.
    Literature detailing the application of extreme learning machines is recent to this decade, and researchers attest to their proficient ability to forecast in financial markets when compared with back-propagation neural networks.
    Milacic, Vujovic, and Miljkovic purposed their research towards comparing extreme learning machines and back-propagation neural networks to forecasting gross domestic product growth rate; the authors determined that extreme learning machines had lower root-mean-squared error, higher correlation between the actual and forecasted series, and that “The extreme learning machine algorithm can be effectively utilized in GDP applications and particularly in the GDP estimations.” 40

    Extreme learning machines require significantly fewer computations and train faster than back-propagation neural networks.
    Li, et al. compared hybrid radial basis function extreme learning machines to back-propagation networks when they were applied towards using news articles and the iterative level estimation method.
    Their results showed that the “RBF-ELM achieved higher prediction accuracy and faster prediction speed when compared to BP-NN.”
    Their article represents the new direction of machine learning and financial forecasting because news articles are now being exploited for high-frequency trading.
    Incremental learning, error reduced model selection, and online learning algorithms exist for extreme learning machines have already been rigorously proven and provided by Feng, et al. and Liang, et al.
    The potential for improving the training and testing time of big data problems are only starting to be explored, and more information can be found at the academic home page of Guang-Bin Huang. 44,45

    Different beliefs regarding forecasting, methods of forecasting, and the results from academic studies of financial markets have been presented within this chapter.
    The Efficient Market and Random Walk Hypotheses are rejected in favor of the tertiary belief that machine learning models can detect nonlinear patterns in stock market time series.
    The experiment proposed within chapter seven, recorded in chapter eight, and discussed in chapter nine propose that these nonlinear patterns can be useful for forecasting daily closing prices of blue chip stocks.
    Recent research suggests that extreme learning machines could be significantly more useful for short-term financial forecasting than back-propagation neural networks.
    The theory of both optimization algorithms are presented in the following two chapters.

\end{document}
